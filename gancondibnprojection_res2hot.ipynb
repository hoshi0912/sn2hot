{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gancondibnprojection_res2hot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoshi0912/sn2hot/blob/master/gancondibnprojection_res2hot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LievEKFXPf2f",
        "colab_type": "code",
        "outputId": "ac05323e-14f7-498e-e3f5-87e52f64995d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Variable\n",
        "from chainer.training import extensions\n",
        "from chainer import initializers\n",
        "from chainer import link\n",
        "from chainer.utils import argument\n",
        "from chainer import variable\n",
        "from chainer.links import EmbedID\n",
        "from chainer import configuration\n",
        "from chainer.functions.normalization import batch_normalization\n",
        "from chainer.functions.array.broadcast import broadcast_to\n",
        "from chainer.initializers import normal\n",
        "from chainer.functions.connection import embed_id\n",
        "from chainer.functions.connection import linear\n",
        "from chainer.links.connection.linear import Linear\n",
        "from chainer.functions.connection import convolution_2d\n",
        "from chainer.links.connection.convolution_2d import Convolution2D\n",
        "import math\n",
        "\n",
        "\n",
        "chainer.print_runtime_info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Platform: Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Chainer: 5.4.0\n",
            "NumPy: 1.16.3\n",
            "CuPy:\n",
            "  CuPy Version          : 5.4.0\n",
            "  CUDA Root             : /usr/local/cuda\n",
            "  CUDA Build Version    : 10000\n",
            "  CUDA Driver Version   : 10000\n",
            "  CUDA Runtime Version  : 10000\n",
            "  cuDNN Build Version   : 7301\n",
            "  cuDNN Version         : 7301\n",
            "  NCCL Build Version    : 2402\n",
            "  NCCL Runtime Version  : 2402\n",
            "iDeep: 2.0.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87tFTAwQUnmE",
        "colab_type": "code",
        "outputId": "9abd7214-9344-4275-b9a1-1fc7b1fa1f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "ws_y = np.array([np.linspace(0, 1, 10)[::-1], np.linspace(0, 1, 10)], dtype=np.float32).T\n",
        "ws_x = np.array([[1, 1]] * 10, dtype=np.float)\n",
        "classes = [np.random.randint(1000), np.random.randint(1000)]\n",
        "ys = np.array([[classes[0], classes[1]]] * 10, dtype=np.int32)\n",
        "ws_n = F.expand_dims(ws_y, 2)\n",
        "print(ws_y.shape)\n",
        "print(ys.shape)\n",
        "print(ws_n)\n",
        "print(ws_x)\n",
        "gamma_c = F.sum(ws_n, 1)\n",
        "print(gamma_c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 2)\n",
            "(10, 2)\n",
            "variable([[[1.        ]\n",
            "           [0.        ]]\n",
            "\n",
            "          [[0.8888889 ]\n",
            "           [0.11111111]]\n",
            "\n",
            "          [[0.7777778 ]\n",
            "           [0.22222222]]\n",
            "\n",
            "          [[0.6666667 ]\n",
            "           [0.33333334]]\n",
            "\n",
            "          [[0.5555556 ]\n",
            "           [0.44444445]]\n",
            "\n",
            "          [[0.44444445]\n",
            "           [0.5555556 ]]\n",
            "\n",
            "          [[0.33333334]\n",
            "           [0.6666667 ]]\n",
            "\n",
            "          [[0.22222222]\n",
            "           [0.7777778 ]]\n",
            "\n",
            "          [[0.11111111]\n",
            "           [0.8888889 ]]\n",
            "\n",
            "          [[0.        ]\n",
            "           [1.        ]]])\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n",
            "variable([[1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]\n",
            "          [1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzypAWA4QDXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "n_epoch = 100  # number of epochs\n",
        "n_hidden = 128  # number of hidden units\n",
        "batchsize = 64  # minibatch size\n",
        "snapshot_interval = 1000 # number of iterations per snapshots\n",
        "display_interval = 100  # number of iterations per display the status\n",
        "gpu_id = 0\n",
        "out_dir = 'result'\n",
        "seed = 0  # random seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vim3C3aaZbbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the CIFAR10 dataset if args.dataset is not specified\n",
        "train, _ = chainer.datasets.get_cifar10(withlabel=True, ndim=3, scale=255.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRElu1xrc4XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v7bvG9gGkSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLinear(Linear):\n",
        "    \"\"\"Linear layer with Spectral Normalization.\n",
        "    Args:\n",
        "        in_size (int): Dimension of input vectors. If ``None``, parameter\n",
        "            initialization will be deferred until the first forward datasets pass\n",
        "            at which time the size will be determined.\n",
        "        out_size (int): Dimension of output vectors.\n",
        "        wscale (float): Scaling factor of the weight matrix.\n",
        "        bias (float): Initial bias value.\n",
        "        nobias (bool): If ``True``, then this function does not use the bias.\n",
        "        initialW (2-D array): Initial weight value. If ``None``, then this\n",
        "            function uses to initialize ``wscale``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        initial_bias (1-D array): Initial bias value. If ``None``, then this\n",
        "            function uses to initialize ``bias``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        use_gamma (bool): If true, apply scalar multiplication to the \n",
        "            normalized weight (i.e. reparameterize).\n",
        "        Ip (int): The number of power iteration for calculating the spcetral \n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso:: :func:`~chainer.functions.linear`\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Weight parameter.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        b (~chainer.Variable): Bias parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size, use_gamma=False, nobias=False,\n",
        "                 initialW=None, initial_bias=None, Ip=1, factor=None):\n",
        "        self.Ip = Ip\n",
        "        self.use_gamma = use_gamma\n",
        "        self.factor = factor\n",
        "        super(SNLinear, self).__init__(\n",
        "            in_size, out_size, nobias, initialW, initial_bias\n",
        "        )\n",
        "        self.u = np.random.normal(size=(1, out_size)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectral Normalized Weight\n",
        "        \"\"\"\n",
        "        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n",
        "        self.u[:] = _u\n",
        "        if hasattr(self, 'gamma'):\n",
        "            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n",
        "        else:\n",
        "            return self.W / sigma\n",
        "\n",
        "    def _initialize_params(self, in_size):\n",
        "        super(SNLinear, self)._initialize_params(in_size)\n",
        "        if self.use_gamma:\n",
        "            _, s, _ = np.linalg.svd(self.W.data)\n",
        "            with self.init_scope():\n",
        "                self.gamma = chainer.Parameter(s[0], (1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies the linear layer.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Batch of input vectors.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Output of the linear layer.\n",
        "        \"\"\"\n",
        "        if self.W.data is None:\n",
        "            self._initialize_params(x.size // x.shape[0])\n",
        "        return linear.linear(x, self.W_bar, self.b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK9BjBhbVJ5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConditionalBatchNormalization(chainer.Chain):\n",
        "    \"\"\"\n",
        "    Conditional Batch Normalization\n",
        "    Args:\n",
        "        size (int or tuple of ints): Size (or shape) of channel\n",
        "            dimensions.\n",
        "        n_cat (int): the number of categories of categorical variable.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability.\n",
        "        dtype (numpy.dtype): Type to use in computing.\n",
        "        use_gamma (bool): If ``True``, use scaling parameter. Otherwise, use\n",
        "            unit(1) which makes no effect.\n",
        "        use_beta (bool): If ``True``, use shifting parameter. Otherwise, use\n",
        "            unit(0) which makes no effect.\n",
        "    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n",
        "          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n",
        "    .. seealso::\n",
        "       :func:`~chainer.functions.batch_normalization`,\n",
        "       :func:`~chainer.functions.fixed_batch_normalization`\n",
        "    Attributes:\n",
        "        gamma (~chainer.Variable): Scaling parameter.\n",
        "        beta (~chainer.Variable): Shifting parameter.\n",
        "        avg_mean (numpy.ndarray or cupy.ndarray): Population mean.\n",
        "        avg_var (numpy.ndarray or cupy.ndarray): Population variance.\n",
        "        N (int): Count of batches given for fine-tuning.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability. This value is added\n",
        "            to the batch variances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, n_cat, decay=0.9, eps=2e-5, dtype=numpy.float32):\n",
        "        super(ConditionalBatchNormalization, self).__init__()\n",
        "        self.avg_mean = numpy.zeros(size, dtype=dtype)\n",
        "        self.register_persistent('avg_mean')\n",
        "        self.avg_var = numpy.zeros(size, dtype=dtype)\n",
        "        self.register_persistent('avg_var')\n",
        "        self.N = 0\n",
        "        self.register_persistent('N')\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.n_cat = n_cat\n",
        "\n",
        "    def __call__(self, x, gamma, beta, **kwargs):\n",
        "        \"\"\"__call__(self, x, c, finetune=False)\n",
        "        Invokes the forward propagation of BatchNormalization.\n",
        "        In training mode, the BatchNormalization computes moving averages of\n",
        "        mean and variance for evaluatino during training, and normalizes the\n",
        "        input using batch statistics.\n",
        "        .. warning::\n",
        "           ``test`` argument is not supported anymore since v2.\n",
        "           Instead, use ``chainer.using_config('train', train)``.\n",
        "           See :func:`chainer.using_config`.\n",
        "        Args:\n",
        "            x (Variable): Input variable.\n",
        "            gamma (Variable): Input variable of gamma of shape\n",
        "            finetune (bool): If it is in the training mode and ``finetune`` is\n",
        "                ``True``, BatchNormalization runs in fine-tuning mode; it\n",
        "                accumulates the input array to compute population statistics\n",
        "                for normalization, and normalizes the input using batch\n",
        "                statistics.\n",
        "        \"\"\"\n",
        "        argument.check_unexpected_kwargs(\n",
        "            kwargs, test='test argument is not supported anymore. '\n",
        "                         'Use chainer.using_config')\n",
        "        finetune, = argument.parse_kwargs(kwargs, ('finetune', False))\n",
        "        with cuda.get_device_from_id(self._device_id):\n",
        "            _gamma = variable.Variable(self.xp.ones(\n",
        "                self.avg_mean.shape, dtype=x.dtype))\n",
        "        with cuda.get_device_from_id(self._device_id):\n",
        "            _beta = variable.Variable(self.xp.zeros(\n",
        "                self.avg_mean.shape, dtype=x.dtype))\n",
        "        if configuration.config.train:\n",
        "            if finetune:\n",
        "                self.N += 1\n",
        "                decay = 1. - 1. / self.N\n",
        "            else:\n",
        "                decay = self.decay\n",
        "            ret = chainer.functions.batch_normalization(x, _gamma, _beta, eps=self.eps, running_mean=self.avg_mean,\n",
        "                                                        running_var=self.avg_var, decay=decay)\n",
        "        else:\n",
        "            # Use running average statistics or fine-tuned statistics.\n",
        "            mean = variable.Variable(self.avg_mean)\n",
        "            var = variable.Variable(self.avg_var)\n",
        "            ret = batch_normalization.fixed_batch_normalization(\n",
        "                x, _gamma, _beta, mean, var, self.eps)\n",
        "        shape = ret.shape\n",
        "        ndim = len(shape)\n",
        "        gamma = F.broadcast_to(F.reshape(gamma, list(gamma.shape) + [1] * (ndim - len(gamma.shape))), shape)\n",
        "        beta = F.broadcast_to(F.reshape(beta, list(beta.shape) + [1] * (ndim - len(beta.shape))), shape)\n",
        "        return gamma * ret + beta\n",
        "\n",
        "\n",
        "def start_finetuning(self):\n",
        "    \"\"\"Resets the population count for collecting population statistics.\n",
        "    This method can be skipped if it is the first time to use the\n",
        "    fine-tuning mode. Otherwise, this method should be called before\n",
        "    starting the fine-tuning mode again.\n",
        "    \"\"\"\n",
        "    self.N = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgctWoUU5_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CategoricalConditionalBatchNormalization(ConditionalBatchNormalization):\n",
        "    \"\"\"\n",
        "    Conditional Batch Normalization\n",
        "    Args:\n",
        "        size (int or tuple of ints): Size (or shape) of channel\n",
        "            dimensions.\n",
        "        n_cat (int): the number of categories of categorical variable.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability.\n",
        "        dtype (numpy.dtype): Type to use in computing.\n",
        "        use_gamma (bool): If ``True``, use scaling parameter. Otherwise, use\n",
        "            unit(1) which makes no effect.\n",
        "        use_beta (bool): If ``True``, use shifting parameter. Otherwise, use\n",
        "            unit(0) which makes no effect.\n",
        "    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n",
        "          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n",
        "    .. seealso::\n",
        "       :func:`~chainer.functions.batch_normalization`,\n",
        "       :func:`~chainer.functions.fixed_batch_normalization`\n",
        "    Attributes:\n",
        "        gamma (~chainer.Variable): Scaling parameter.\n",
        "        beta (~chainer.Variable): Shifting parameter.\n",
        "        avg_mean (numpy.ndarray or cupy.ndarray): Population mean.\n",
        "        avg_var (numpy.ndarray or cupy.ndarray): Population variance.\n",
        "        N (int): Count of batches given for fine-tuning.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability. This value is added\n",
        "            to the batch variances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, n_cat, decay=0.9, eps=2e-5, dtype=numpy.float32,\n",
        "                 initial_gamma=None, initial_beta=None):\n",
        "        super(CategoricalConditionalBatchNormalization, self).__init__(\n",
        "            size=size, n_cat=n_cat, decay=decay, eps=eps, dtype=dtype)\n",
        "\n",
        "        with self.init_scope():\n",
        "            if initial_gamma is None:\n",
        "                initial_gamma = 1\n",
        "            initial_gamma = initializers._get_initializer(initial_gamma)\n",
        "            initial_gamma.dtype = dtype\n",
        "            self.gammas = EmbedID(n_cat, size, initialW=initial_gamma)\n",
        "            if initial_beta is None:\n",
        "                initial_beta = 0\n",
        "            initial_beta = initializers._get_initializer(initial_beta)\n",
        "            initial_beta.dtype = dtype\n",
        "            self.betas = EmbedID(n_cat, size, initialW=initial_beta)\n",
        "\n",
        "    def __call__(self, x, c, finetune=False, **kwargs):\n",
        "        \"\"\"__call__(self, x, c, finetune=False)\n",
        "        Invokes the forward propagation of BatchNormalization.\n",
        "        In training mode, the BatchNormalization computes moving averages of\n",
        "        mean and variance for evaluatino during training, and normalizes the\n",
        "        input using batch statistics.\n",
        "        .. warning::\n",
        "           ``test`` argument is not supported anymore since v2.\n",
        "           Instead, use ``chainer.using_config('train', train)``.\n",
        "           See :func:`chainer.using_config`.\n",
        "        Args:\n",
        "            x (Variable): Input variable.\n",
        "            c (Variable): Input variable for conditioning gamma and beta\n",
        "            finetune (bool): If it is in the training mode and ``finetune`` is\n",
        "                ``True``, BatchNormalization runs in fine-tuning mode; it\n",
        "                accumulates the input array to compute population statistics\n",
        "                for normalization, and normalizes the input using batch\n",
        "                statistics.\n",
        "        \"\"\"\n",
        "        weights, = argument.parse_kwargs(kwargs, ('weights', None))\n",
        "        if c.ndim == 2 and weights is not None:\n",
        "            _gamma_c = self.gammas(c)\n",
        "            _beta_c = self.betas(c)\n",
        "            _gamma_c = F.broadcast_to(F.expand_dims(weights, 2), _gamma_c.shape) * _gamma_c \n",
        "            _beta_c = F.broadcast_to(F.expand_dims(weights, 2), _beta_c.shape) * _beta_c\n",
        "            gamma_c = F.sum(_gamma_c, 1) \n",
        "            beta_c = F.sum(_beta_c, 1)\n",
        "        if c.ndim == 2:\n",
        "            gamma_c = F.sum(self.gammas(c))\n",
        "            beta_c = F.sum(self.betas(c))\n",
        "        else:\n",
        "            gamma_c = self.gammas(c)\n",
        "            beta_c = self.betas(c)\n",
        "        return super(CategoricalConditionalBatchNormalization, self).__call__(x, gamma_c, beta_c, **kwargs)\n",
        "\n",
        "\n",
        "def start_finetuning(self):\n",
        "    \"\"\"Resets the population count for collecting population statistics.\n",
        "    This method can be skipped if it is the first time to use the\n",
        "    fine-tuning mode. Otherwise, this method should be called before\n",
        "    starting the fine-tuning mode again.\n",
        "    \"\"\"\n",
        "    self.N = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iZSGpenRcne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _upsample(x):\n",
        "    h, w = x.shape[2:]\n",
        "    return F.unpooling_2d(x, 2, outsize=(h * 2, w * 2))\n",
        "\n",
        "\n",
        "def upsample_conv(x, conv):\n",
        "    return conv(_upsample(x))\n",
        "\n",
        "\n",
        "class GBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1,\n",
        "                 activation=F.relu, upsample=False, n_classes=0):\n",
        "        super(GBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        self.upsample = upsample\n",
        "        self.learnable_sc = in_channels != out_channels or upsample\n",
        "        hidden_channels = out_channels if hidden_channels is None else hidden_channels\n",
        "        self.n_classes = n_classes\n",
        "        with self.init_scope():\n",
        "            self.c1 = L.Convolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = L.Convolution2D(hidden_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            if n_classes > 0:\n",
        "                self.b1 = CategoricalConditionalBatchNormalization(in_channels, n_cat=n_classes)\n",
        "                self.b2 = CategoricalConditionalBatchNormalization(hidden_channels, n_cat=n_classes)\n",
        "            else:\n",
        "                self.b1 = L.BatchNormalization(in_channels)\n",
        "                self.b2 = L.BatchNormalization(hidden_channels)\n",
        "            if self.learnable_sc:\n",
        "                self.c_sc = L.Convolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x, y=None, z=None, **kwargs):\n",
        "        h = x\n",
        "        h = self.b1(h, y, **kwargs) if y is not None else self.b1(h, **kwargs)\n",
        "        h = self.activation(h)\n",
        "        h = upsample_conv(h, self.c1) if self.upsample else self.c1(h)\n",
        "        h = self.b2(h, y, **kwargs) if y is not None else self.b2(h, **kwargs)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = upsample_conv(x, self.c_sc) if self.upsample else self.c_sc(x)\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __call__(self, x, y=None, z=None, **kwargs):\n",
        "        return self.residual(x, y, z, **kwargs) + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBi239mYQCjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_hidden = 128, n_classes=10, bottom_width=4, bottom_height=4, ch=256, wscale=0.02, activation=F.relu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.ch = ch\n",
        "        self.bottom_width = bottom_width\n",
        "        self.bottom_height = bottom_height\n",
        "        self.activation = activation\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.GlorotUniform()\n",
        "            self.l1 = L.Linear(n_hidden, (bottom_width ** 2) * ch, initialW=w)\n",
        "            self.block2 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block3 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block4 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block5 = GBlock(ch * 4, ch * 2, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block6 = GBlock(ch * 2, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.b7 = L.BatchNormalization(ch)\n",
        "            self.l7 = L.Convolution2D(ch, 3, ksize=3, stride=1, pad=1, initialW=w)\n",
        "\n",
        "    def make_hidden(self, batchsize):\n",
        "        #np.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1)).astype(np.float32)\n",
        "        return np.random.randn(batchsize, n_hidden).astype(np.float32)\n",
        "\n",
        "    def __call__(self, z, c):\n",
        "        #h = F.reshape(F.relu(self.bn0(self.l0(z))),(len(z), -1, self.bottom_width, self.bottom_height))\n",
        "        h = self.l1(z)\n",
        "        h = F.reshape(h,(h.shape[0], -1, self.bottom_width, self.bottom_height))\n",
        "        h = self.block2(h, c)\n",
        "        h = self.block3(h, c)\n",
        "        h = self.block4(h, c)\n",
        "        #h = self.block5(h, c)\n",
        "        #h = self.block6(h, c)\n",
        "        h = self.b7(h)\n",
        "        h = self.activation(h)\n",
        "        h = F.tanh(self.l7(h))\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRWiOnzYpeg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _l2normalize(v, eps=1e-12):\n",
        "    norm = cuda.reduce('T x', 'T out',\n",
        "                       'x * x', 'a + b', 'out = sqrt(a)', 0,\n",
        "                       'norm_sn')\n",
        "    div = cuda.elementwise('T x, T norm, T eps',\n",
        "                           'T out',\n",
        "                           'out = x / (norm + eps)',\n",
        "                           'div_sn')\n",
        "    return div(v, norm(v), eps)\n",
        "\n",
        "\n",
        "def max_singular_value(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    Apply power iteration for the weight parameter\n",
        "    \"\"\"\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"The number of power iterations should be positive integer\")\n",
        "\n",
        "    xp = cuda.get_array_module(W.data)\n",
        "    if u is None:\n",
        "        u = xp.random.normal(size=(1, W.shape[0])).astype(xp.float32)\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = _l2normalize(xp.dot(_u, W.data), eps=1e-12)\n",
        "        _u = _l2normalize(xp.dot(_v, W.data.transpose()), eps=1e-12)\n",
        "    sigma = F.sum(F.linear(_u, F.transpose(W)) * _v)\n",
        "    return sigma, _u, _v\n",
        "\n",
        "\n",
        "def max_singular_value_fully_differentiable(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    Apply power iteration for the weight parameter (fully differentiable version)\n",
        "    \"\"\"\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"The number of power iterations should be positive integer\")\n",
        "\n",
        "    xp = cuda.get_array_module(W.data)\n",
        "    if u is None:\n",
        "        u = xp.random.normal(size=(1, W.shape[0])).astype(xp.float32)\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = F.normalize(F.matmul(_u, W), eps=1e-12)\n",
        "        _u = F.normalize(F.matmul(_v, F.transpose(W)), eps=1e-12)\n",
        "    _u = F.matmul(_v, F.transpose(W))\n",
        "    norm = F.sqrt(F.sum(_u ** 2))\n",
        "    return norm, _l2normalize(_u.data), _v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Amo53HLpLZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNEmbedID(link.Link):\n",
        "    \"\"\"Efficient linear layer for one-hot input.\n",
        "    This is a link that wraps the :func:`~chainer.functions.embed_id` function.\n",
        "    This link holds the ID (word) embedding matrix ``W`` as a parameter.\n",
        "    Args:\n",
        "        in_size (int): Number of different identifiers (a.k.a. vocabulary\n",
        "            size).\n",
        "        out_size (int): Size of embedding vector.\n",
        "        initialW (2-D array): Initial weight value. If ``None``, then the\n",
        "            matrix is initialized from the standard normal distribution.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        ignore_label (int or None): If ``ignore_label`` is an int value,\n",
        "            ``i``-th column of return value is filled with ``0``.\n",
        "        Ip (int): The number of power iteration for calculating the spcetral\n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso:: :func:`chainer.functions.embed_id`\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Embedding parameter matrix.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    ignore_label = None\n",
        "\n",
        "    def __init__(self, in_size, out_size, initialW=None, ignore_label=None, Ip=1, factor=None):\n",
        "        super(SNEmbedID, self).__init__()\n",
        "        self.ignore_label = ignore_label\n",
        "        self.Ip = Ip\n",
        "        self.factor = factor\n",
        "        with self.init_scope():\n",
        "            if initialW is None:\n",
        "                initialW = normal.Normal(1.0)\n",
        "            self.W = variable.Parameter(initialW, (in_size, out_size))\n",
        "\n",
        "        self.u = np.random.normal(size=(1, in_size)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectral Normalized Weight\n",
        "        \"\"\"\n",
        "        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n",
        "        self.u[:] = _u\n",
        "        return self.W / sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Extracts the word embedding of given IDs.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Batch vectors of IDs.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Batch of corresponding embeddings.\n",
        "        \"\"\"\n",
        "        return embed_id.embed_id(x, self.W_bar, ignore_label=self.ignore_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykqsePhTOBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNConvolution2D(Convolution2D):\n",
        "    \"\"\"Two-dimensional convolutional layer with spectral normalization.\n",
        "    This link wraps the :func:`~chainer.functions.convolution_2d` function and\n",
        "    holds the filter weight and bias vector as parameters.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels of input arrays. If ``None``,\n",
        "            parameter initialization will be deferred until the first forward\n",
        "            datasets pass at which time the size will be determined.\n",
        "        out_channels (int): Number of channels of output arrays.\n",
        "        ksize (int or pair of ints): Size of filters (a.k.a. kernels).\n",
        "            ``ksize=k`` and ``ksize=(k, k)`` are equivalent.\n",
        "        stride (int or pair of ints): Stride of filter applications.\n",
        "            ``stride=s`` and ``stride=(s, s)`` are equivalent.\n",
        "        pad (int or pair of ints): Spatial padding width for input arrays.\n",
        "            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n",
        "        wscale (float): Scaling factor of the initial weight.\n",
        "        bias (float): Initial bias value.\n",
        "        nobias (bool): If ``True``, then this link does not use the bias term.\n",
        "        initialW (4-D array): Initial weight value. If ``None``, then this\n",
        "            function uses to initialize ``wscale``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        initial_bias (1-D array): Initial bias value. If ``None``, then this\n",
        "            function uses to initialize ``bias``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        use_gamma (bool): If true, apply scalar multiplication to the \n",
        "            normalized weight (i.e. reparameterize).\n",
        "        Ip (int): The number of power iteration for calculating the spcetral \n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso::\n",
        "       See :func:`chainer.functions.convolution_2d` for the definition of\n",
        "       two-dimensional convolution.\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Weight parameter.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        b (~chainer.Variable): Bias parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, ksize, stride=1, pad=0,\n",
        "                 nobias=False, initialW=None, initial_bias=None, use_gamma=False, Ip=1, factor=None):\n",
        "        self.Ip = Ip\n",
        "        self.use_gamma = use_gamma\n",
        "        self.factor = factor\n",
        "        super(SNConvolution2D, self).__init__(\n",
        "            in_channels, out_channels, ksize, stride, pad,\n",
        "            nobias, initialW, initial_bias)\n",
        "        self.u = np.random.normal(size=(1, out_channels)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectrally Normalized Weight\n",
        "        \"\"\"\n",
        "        W_mat = self.W.reshape(self.W.shape[0], -1)\n",
        "        sigma, _u, _ = max_singular_value(W_mat, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1, 1, 1)), self.W.shape)\n",
        "        if chainer.config.train:\n",
        "            # Update estimated 1st singular vector\n",
        "            self.u[:] = _u\n",
        "        if hasattr(self, 'gamma'):\n",
        "            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n",
        "        else:\n",
        "            return self.W / sigma\n",
        "\n",
        "    def _initialize_params(self, in_size):\n",
        "        super(SNConvolution2D, self)._initialize_params(in_size)\n",
        "        if self.use_gamma:\n",
        "            W_mat = self.W.data.reshape(self.W.shape[0], -1)\n",
        "            _, s, _ = np.linalg.svd(W_mat)\n",
        "            with self.init_scope():\n",
        "                self.gamma = chainer.Parameter(s[0], (1, 1, 1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies the convolution layer.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Input image.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Output of the convolution.\n",
        "        \"\"\"\n",
        "        if self.W.data is None:\n",
        "            self._initialize_params(x.shape[1])\n",
        "        return convolution_2d.convolution_2d(\n",
        "            x, self.W_bar, self.b, self.stride, self.pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM4w8vMKS8in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return F.average_pooling_2d(x, 2)\n",
        "\n",
        "\n",
        "class DBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1,\n",
        "                 activation=F.relu, downsample=False):\n",
        "        super(DBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        self.downsample = downsample\n",
        "        self.learnable_sc = (in_channels != out_channels) or downsample\n",
        "        hidden_channels = in_channels if hidden_channels is None else hidden_channels\n",
        "        with self.init_scope():\n",
        "            self.c1 = SNConvolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = SNConvolution2D(hidden_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            if self.learnable_sc:\n",
        "                self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x):\n",
        "        h = x\n",
        "        h = self.activation(h)\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        if self.downsample:\n",
        "            h = _downsample(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = self.c_sc(x)\n",
        "            if self.downsample:\n",
        "                return _downsample(x)\n",
        "            else:\n",
        "                return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.residual(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class OptimizedBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, ksize=3, pad=1, activation=F.relu):\n",
        "        super(OptimizedBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        with self.init_scope():\n",
        "            self.c1 = SNConvolution2D(in_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = SNConvolution2D(out_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x):\n",
        "        h = x\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        h = _downsample(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        return self.c_sc(_downsample(x))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.residual(x) + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagog6cdQNzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(h, sigma=0.2):\n",
        "    xp = cuda.get_array_module(h.data)\n",
        "    if chainer.config.train:\n",
        "        return h + sigma * xp.random.randn(*h.shape)\n",
        "    else:\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY-831NFQMJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, bottom_width=4, bottom_height=4, ch=128, n_classes=10, wscale=0.02, activation=F.relu):\n",
        "        w = chainer.initializers.GlorotUniform()\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.activation = activation\n",
        "        with self.init_scope():\n",
        "            self.block1 = OptimizedBlock(3, ch)\n",
        "            self.block2 = DBlock(ch, ch, activation=activation, downsample=True)\n",
        "            self.block3 = DBlock(ch, ch, activation=activation, downsample=False)\n",
        "            self.block4 = DBlock(ch, ch, activation=activation, downsample=False)\n",
        "            self.block5 = DBlock(ch * 4, ch * 8, activation=activation, downsample=True)\n",
        "            self.block6 = DBlock(ch * 8, ch * 8, activation=activation, downsample=False)\n",
        "            self.l7 = SNLinear(ch, 1, initialW=w, nobias=True)\n",
        "            if n_classes > 0:\n",
        "                self.l_y = SNEmbedID(n_classes, ch, initialW=w)\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        h = x\n",
        "        h = self.block1(h)\n",
        "        h = self.block2(h)\n",
        "        h = self.block3(h)\n",
        "        h = self.block4(h)\n",
        "\n",
        "        #h = self.block5(h)\n",
        "        #h = self.block6(h)\n",
        "        h = self.activation(h)\n",
        "        \n",
        "        h = F.sum(h, axis=(2, 3))  # Global pooling\n",
        "        \n",
        "        output = self.l7(h)\n",
        "        \n",
        "        if y is not None:\n",
        "            w_y = F.sum(F.mean(self.l_y(y)))\n",
        "            output += F.sum(w_y * h, axis=1, keepdims=True)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMBOImQjZeaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(n_hidden=n_hidden)\n",
        "dis = Discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaZR2PeVZjcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup an optimizer\n",
        "def make_optimizer(model, alpha=0.0002, beta1=0.0, beta2=0.9):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1, beta2=beta2)\n",
        "    optimizer.setup(model)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNKJ1wPRQPZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = make_optimizer(gen)\n",
        "opt_dis = make_optimizer(dis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--UyAQApQWtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import chainer.backends.cuda\n",
        "\n",
        "def out_generated_image(gen, dis, rows, cols, seed, dst):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        np.random.seed(seed)\n",
        "        n_images = rows * cols\n",
        "        xp = gen.xp\n",
        "        classes = np.arange(0, 10, dtype=np.int32)\n",
        "        #test = np.full(10, 9, dtype=np.int32)\n",
        "        test = 9\n",
        "        for c in classes:\n",
        "            y = xp.asarray([[c, test]] * n_images, dtype=xp.int32)\n",
        "            z = Variable(xp.asarray(gen.make_hidden(n_images)))\n",
        "\n",
        "            with chainer.using_config('train', False):\n",
        "                x = gen(z, y)\n",
        "            x = chainer.backends.cuda.to_cpu(x.data)\n",
        "            np.random.seed()\n",
        "\n",
        "            x = np.asarray(np.clip(x * 255.0, 0.0, 255.0), dtype=np.uint8)\n",
        "            _, _, H, W = x.shape\n",
        "            x = x.reshape((rows, cols, 3, H, W))\n",
        "            x = x.transpose(0, 3, 1, 4, 2)\n",
        "            x = x.reshape((rows * H, cols * W, 3))\n",
        "\n",
        "            preview_dir = '{}/preview'.format(dst)\n",
        "            preview_path = preview_dir +\\\n",
        "                '/image{}{:0>8}.png'.format(c, trainer.updater.iteration)\n",
        "            if not os.path.exists(preview_dir):\n",
        "                os.makedirs(preview_dir)\n",
        "            Image.fromarray(x).save(preview_path)\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjEb6qZrQ-uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_hinge_dis(dis_fake, dis_real):\n",
        "    loss = F.mean(F.relu(1. - dis_real))\n",
        "    loss += F.mean(F.relu(1. + dis_fake))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def loss_hinge_gen(dis_fake):\n",
        "    loss = -F.mean(dis_fake)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis = kwargs.pop('models')\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "        self.xp = self.gen.xp\n",
        "        self.loss_gen = loss_hinge_gen\n",
        "        self.loss_dis = loss_hinge_dis\n",
        "\n",
        "    def loss_dis(self, dis, y_fake, y_real):\n",
        "        batchsize = len(y_fake)\n",
        "        L1 = F.sum(F.softplus(-y_real)) /  batchsize\n",
        "        L2 = F.sum(F.softplus(y_fake)) /  batchsize\n",
        "        \n",
        "        loss = L1 + L2\n",
        "        chainer.report({'loss': loss}, dis)\n",
        "        return loss\n",
        "\n",
        "    def loss_gen(self, gen, y_fake):\n",
        "        batchsize = len(y_fake)\n",
        "        loss = F.sum(F.softplus(-y_fake)) /  batchsize\n",
        "        chainer.report({'loss': loss}, gen)\n",
        "        return loss\n",
        "    \n",
        "    def loss_hinge_dis(self, dis, dis_fake, dis_real):\n",
        "        batchsize = len(dis_fake)\n",
        "        loss = F.mean(F.relu(1. - dis_real))\n",
        "        loss += F.mean(F.relu(1. + dis_fake))\n",
        "        chainer.report({'loss': loss}, dis)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss_hinge_gen(self, gen, dis_fake):\n",
        "        batchsize = len(dis_fake)\n",
        "        loss = -F.mean(dis_fake)\n",
        "        chainer.report({'loss': loss}, gen)\n",
        "        return loss\n",
        "\n",
        "    def make_onehot(self, classes):\n",
        "        return self.xp.eye(10, dtype='f')[classes]\n",
        "\n",
        "    def update_core(self):\n",
        "        gen_optimizer = self.get_optimizer('gen')\n",
        "        dis_optimizer = self.get_optimizer('dis')\n",
        "        gen, dis = self.gen, self.dis\n",
        "        \n",
        "        for i in range(5):\n",
        "          if i == 0:\n",
        "            z = Variable(self.xp.asarray(gen.make_hidden(128)))\n",
        "            onehot = self.xp.random.randint(low=0, high=10, size=(128, 2)).astype(self.xp.int32)\n",
        "            x_fake = gen(z, onehot)\n",
        "            y_fake = dis(x_fake, onehot)\n",
        "            loss_gen = self.loss_gen(dis_fake=y_fake)\n",
        "            gen.cleargrads()\n",
        "            loss_gen.backward()\n",
        "            gen_optimizer.update()\n",
        "            chainer.reporter.report({'loss_gen': loss_gen})\n",
        "            \n",
        "          batch = self.get_iterator('main').next()\n",
        "          batchsize = len(batch)\n",
        "          images = [batch[i][0] for i in range(batchsize)]\n",
        "          labels = [batch[i][1] for i in range(batchsize)]\n",
        "        #test = np.array(images).repeat(2, axis=2)\n",
        "        #test = np.array(test).repeat(2, axis=3)\n",
        "          #test = [np.concatenate((batch[i][0],batch[i+1][0]), axis=1) for i in range(batchsize -1)]\n",
        "          \n",
        "          x_real = Variable(self.converter(images, self.device)) / 255.\n",
        "        \n",
        "          x_label = Variable(self.converter(labels, self.device))\n",
        "        \n",
        "          xp_label = chainer.cuda.get_array_module(x_label.data)\n",
        "          labels = xp_label.asarray(labels)\n",
        "          test = self.xp.asarray([9] * batchsize, dtype=self.xp.int32)\n",
        "          labels = F.concat((F.expand_dims(labels, 1),F.expand_dims(test, 1)), axis=1)\n",
        "        \n",
        "          classes = self.xp.random.random_integers(0, 9, len(batch))\n",
        "          #onehot = chainer.Variable(self.make_onehot(classes))\n",
        "          classes = chainer.Variable(classes)\n",
        "        \n",
        "          xp = chainer.backends.cuda.get_array_module(x_real.data)\n",
        "        \n",
        "         \n",
        "        \n",
        "        \n",
        "          y_real = dis(x_real, labels)\n",
        "          x_fake = gen(z, onehot)\n",
        "          y_fake = dis(x_fake, onehot)\n",
        "          x_fake.unchain_backward()\n",
        "\n",
        "          loss_dis = self.loss_dis(dis_fake=y_fake, dis_real=y_real)\n",
        "          dis.cleargrads()\n",
        "          loss_dis.backward()\n",
        "          dis_optimizer.update()\n",
        "          chainer.reporter.report({'loss_dis': loss_dis})\n",
        "        \n",
        "          \n",
        "          \n",
        "\n",
        "          #dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)\n",
        "          #gen_optimizer.update(self.loss_gen, gen, y_fake)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbJnF8rKcxx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "      models=(gen, dis),\n",
        "      iterator=train_iter,\n",
        "      optimizer={\n",
        "          'gen': opt_gen, 'dis': opt_dis},\n",
        "      device=gpu_id)\n",
        "trainer = chainer.training.Trainer(updater, (n_epoch, 'epoch'), out=out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddKM13U7QdFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')\n",
        "trainer.extend(\n",
        "    extensions.snapshot(filename='snapshot_iter_{.updater.iteration}.npz'),\n",
        "    trigger=snapshot_interval)\n",
        "trainer.extend(extensions.snapshot_object(\n",
        "    gen, 'gen_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n",
        "trainer.extend(extensions.snapshot_object(\n",
        "    dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n",
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'loss_dis', 'loss_gen',\n",
        "]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=100))\n",
        "trainer.extend(\n",
        "    out_generated_image(\n",
        "        gen, dis,\n",
        "        5, 5, seed, out_dir),\n",
        "    trigger=snapshot_interval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgCeR7tQlx9",
        "colab_type": "code",
        "outputId": "8cf8663b-8bd8-4fc6-c8df-bb489dd42f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "# Run the training\n",
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       iteration   loss_dis    loss_gen  \n",
            "\u001b[J0           100         0.0896436   2.64848     \n",
            "\u001b[J     total [..................................................]  0.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "       100 iter, 0 epoch / 100 epochs\n",
            "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
            "\u001b[4A\u001b[J1           200         0.022091    2.9954      \n",
            "\u001b[J     total [..................................................]  1.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "       200 iter, 1 epoch / 100 epochs\n",
            "     0.703 iters/sec. Estimated time to finish: 6:05:41.765827.\n",
            "\u001b[4A\u001b[J1           300         0           2.05401     \n",
            "\u001b[J     total [..................................................]  1.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "       300 iter, 1 epoch / 100 epochs\n",
            "   0.70268 iters/sec. Estimated time to finish: 6:03:29.456575.\n",
            "\u001b[4A\u001b[J"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-266-041e2033e90a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-263-9eca89aeccb9>\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0monehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mx_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/random/sample.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(low, high, size, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[1;32m    111\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/random/generator.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, low, high, size, dtype)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0;34m'Sampling from a range whose extent is larger than int32 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 'range is currently not supported')\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cupy/random/generator.py\u001b[0m in \u001b[0;36m_interval\u001b[0;34m(self, mx, size)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;31m# If the sampling has finished in the first iteration,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;31m# just return the sample.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0mn_rem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC7hHHIZwXmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "ls result/preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whbljy_pwfpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, display_png\n",
        "import glob\n",
        "\n",
        "image_files = sorted(glob.glob(out_dir + '/preview/*.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hMW0qYmwkVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_png(Image(image_files[0]))  # first snapshot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3PWjcUjwljh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_png(Image(image_files[-1]))  # last snapshot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG2TN2IbVG51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}